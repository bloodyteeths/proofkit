name: CI Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Daily validation at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: "3.11"
  COVERAGE_THRESHOLD: 92

jobs:
  # Primary CI job with unit tests, linting, and quality checks
  ci:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 2

    - name: Setup Python with Cache
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt

    - name: Cache wheel dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-wheels-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-wheels-

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip wheel
        pip install --use-pep517 -r requirements.txt
        pip install --use-pep517 -r requirements-dev.txt

    - name: Code Quality Checks
      run: |
        # Fast linting and formatting checks
        black --check --diff .
        isort --check-only --diff .
        flake8 . --select=E9,F63,F7,F82 --show-source --statistics
        
    - name: Type Check
      run: mypy . --cache-dir=.mypy_cache

    - name: Security Scan
      run: |
        safety check --short-report
        bandit -r . -ll -x tests/,storage/,venv/ -f json -o bandit-report.json || true

    - name: Unit Tests with Coverage
      run: |
        pytest -n auto --tb=short --maxfail=3 \
          --cov=. --cov-report=xml --cov-report=html \
          --cov-fail-under=${{ env.COVERAGE_THRESHOLD }} \
          --junitxml=pytest-results.xml

    - name: Critical Module Coverage Check
      run: |
        # Enforce 100% coverage for metrics_* and render_* modules
        coverage report --include="*/core/metrics_*.py" --include="*/core/render_*.py" --fail-under=100

    - name: Upload Coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

    - name: Upload Test Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ github.run_id }}
        path: |
          pytest-results.xml
          coverage.xml
          htmlcov/
          bandit-report.json
        retention-days: 7

  # Acceptance tests with industry validations  
  acceptance:
    runs-on: ubuntu-latest
    needs: ci
    timeout-minutes: 30
    strategy:
      matrix:
        industry: ["autoclave", "coldchain", "concrete", "haccp", "powder", "sterile"]
      fail-fast: false

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Setup Python with Cache
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          requirements-dev.txt

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip wheel
        pip install --use-pep517 -r requirements.txt
        pip install --use-pep517 -r requirements-dev.txt

    - name: Create Test Directories
      run: |
        mkdir -p audit/fixtures/{autoclave,coldchain,concrete,haccp,powder,sterile}
        mkdir -p realworld/{autoclave,coldchain,concrete,sterile,oven-shape}
        mkdir -p validation_campaign

    - name: Run Acceptance Tests - ${{ matrix.industry }}
      run: |
        pytest tests/acceptance/test_${{ matrix.industry }}_acceptance.py -v --tb=short --timeout=300

    - name: Upload Acceptance Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: acceptance-${{ matrix.industry }}-${{ github.run_id }}
        path: |
          .pytest_cache/
          test-results-*.xml
        retention-days: 7

  # Campaign validation with registry and differential checks
  campaign:
    runs-on: ubuntu-latest
    needs: ci
    timeout-minutes: 25

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Setup Python with Cache
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip wheel
        pip install --use-pep517 -r requirements.txt
        pip install --use-pep517 -r requirements-dev.txt

    - name: Registry Sanity Check
      run: |
        python scripts/registry_sanity.py \
          --registry validation_campaign/registry.yaml \
          --output registry_sanity_report.json \
          --verbose

    - name: Differential Validation
      run: |
        python scripts/diff_check.py \
          --tolerance 0.05 \
          --examples-dir audit/fixtures \
          --output diff_check_report.json \
          --log-level INFO

    - name: Run Validation Campaign
      run: |
        python -m cli.validate_campaign \
          --industries powder,autoclave,coldchain \
          --max-per-industry 2 \
          --save

    - name: Check Campaign Accuracy
      run: |
        python -c "
        import json
        with open('validation_campaign/report_consistency.json') as f:
            report = json.load(f)
        accuracy = report.get('accuracy', 0)
        violations = len(report.get('tolerance_violations', []))
        print(f'Accuracy: {accuracy}%')
        print(f'Violations: {violations}')
        if accuracy < 95:
            print('::error::Campaign accuracy below 95%')
            exit(1)
        if violations > 0:
            print('::error::Found tolerance violations')
            exit(1)
        "

    - name: Upload Campaign Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: campaign-results-${{ github.run_id }}
        path: |
          registry_sanity_report.json
          diff_check_report.json
          validation_campaign/report_consistency.json
        retention-days: 30

  # Live smoke tests (only on main branch pushes and schedule)
  smoke:
    runs-on: ubuntu-latest
    needs: [ci, acceptance, campaign] 
    if: github.ref == 'refs/heads/main' || github.event_name == 'schedule'
    timeout-minutes: 10

    steps:
    - name: Health Check
      run: |
        response=$(curl -s -w "%{http_code}|%{time_total}" \
          --max-time 10 --retry 2 --retry-delay 3 \
          "https://proofkit.net/health" || echo "000|0")
        
        http_code=$(echo "$response" | cut -d'|' -f1)
        response_time=$(echo "$response" | cut -d'|' -f2)
        
        if [ "$http_code" != "200" ]; then
          echo "❌ Health check failed: $http_code"
          exit 1
        fi
        
        if (( $(echo "$response_time > 5" | bc -l) )); then
          echo "⚠️ Slow response: ${response_time}s"
          exit 1
        fi
        
        echo "✅ Health check passed (${response_time}s)"

    - name: API Endpoints Test  
      run: |
        # Test presets endpoint
        response=$(curl -s -w "%{http_code}" --max-time 15 \
          -H "Accept: application/json" \
          "https://proofkit.net/api/presets" || echo "000")
        
        if [ "$response" != "200" ]; then
          echo "❌ API presets failed: $response"
          exit 1
        fi
        
        echo "✅ API endpoints accessible"

  # Audit and release validation
  audit:
    runs-on: ubuntu-latest
    needs: [ci, acceptance, campaign]
    if: github.ref == 'refs/heads/main'
    timeout-minutes: 20

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Setup Python with Cache
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip wheel
        pip install --use-pep517 -r requirements.txt
        pip install --use-pep517 -r requirements-dev.txt

    - name: Create Storage Directory
      run: mkdir -p storage

    - name: Flaky Test Guard
      run: python scripts/flaky_guard.py
      env:
        MAX_RETRIES: 2
        PYTEST_ARGS: "--tb=short -x"

    - name: Run Audit Framework
      run: make audit
      env:
        AUDIT_VERBOSE: true

    - name: Determinism Check
      run: make audit-determinism

    - name: Performance Benchmark
      run: |
        make audit-benchmark
        # Fail if processing exceeds 2s for standard datasets
        make benchmark | tee benchmark.log
        if grep -q "Average processing time: [2-9]\." benchmark.log; then
          echo "❌ Performance regression detected"
          exit 1
        fi

    - name: Docker Build Test
      run: |
        make build
        docker run -d -p 8000:8000 --name proofkit-test proofkit
        sleep 5
        curl -f http://localhost:8000/status || exit 1
        docker stop proofkit-test && docker rm proofkit-test

    - name: Release Validation
      run: |
        make release-validate
        make release-check-prod

    - name: Upload Audit Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: audit-results-${{ github.run_id }}
        path: |
          audit-report.json
          audit-report.html
          release-report.json
          release-report.html
          benchmark.log
        retention-days: 30

  # Final status check and summary
  status:
    runs-on: ubuntu-latest
    needs: [ci, acceptance, campaign, smoke, audit]
    if: always()
    
    steps:
    - name: Pipeline Summary
      run: |
        echo "# CI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Branch/Tag:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY  
        echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "**Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Job Status" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| CI | ${{ needs.ci.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Acceptance | ${{ needs.acceptance.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Campaign | ${{ needs.campaign.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Smoke Tests | ${{ needs.smoke.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Audit | ${{ needs.audit.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
        
        # Overall status
        overall_status="✅ SUCCESS"
        if [[ "${{ needs.ci.result }}" != "success" || 
              "${{ needs.acceptance.result }}" != "success" || 
              "${{ needs.campaign.result }}" != "success" ]]; then
          overall_status="❌ FAILURE"
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## $overall_status" >> $GITHUB_STEP_SUMMARY

    - name: Check Critical Jobs
      run: |
        # Fail if any critical job failed
        if [[ "${{ needs.ci.result }}" != "success" || 
              "${{ needs.acceptance.result }}" != "success" || 
              "${{ needs.campaign.result }}" != "success" ]]; then
          echo "❌ Critical jobs failed"
          exit 1
        fi
        echo "✅ All critical jobs passed"